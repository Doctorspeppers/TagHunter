# -*- coding: utf-8 -*-
"""TCC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yhYgEBFBEDGmrnUFXyZ_DSSO3RWakKPG
"""

from bs4 import BeautifulSoup
import requests
import json
import datetime

"""
TAG HUNTER
@params str url
@params str sitename
@param list tagContent[str(tag), dict(features)]
@params list findparams [[str(tag), dict(features), (optional)str(parameter to get the content)], [str(tag), dict(features), (optional)str(parameter to get the content)], [str(tag), dict(features), (optional)str(parameter to get the content)]]
"""

class TagHunter:
  
  def __init__(self, url, sitename, tagContent, maxdeep=None):
    self.sitename = sitename
    self.url = []
    self.urlInicial = url
    self.forUrls = []
    self.tagContent = tagContent
    self.url.append(url)
    self.pages = []
    self.content = []
    self.dirtyContent = []
    self.urls_ja_escaneadas = []
    self.confirmPage = True
    self.confirmAnalyze = True
    self.confirmUrls = True
    self.deep = []
    self.maxdeep = maxdeep

  def main(self):
    while self.confirmPage == True or self.confirmAnalyze == True:
      if self.maxdeep == None:
        self.confirmPage = self.GetPages()
        self.confirmAnalyze = self.Analyze()
        pass
      elif self.maxdeep > len(self.deep):
        self.confirmPage = self.GetPages()
        self.confirmAnalyze = self.Analyze()
        pass
      elif self.maxdeep <= len(self.deep):
        break
    self.confirmAnalyze = self.Analyze()
    
  def Analyze(self):
    try:
      if len(self.pages) != 0:
        pageForAnalyze = self.pages.pop()
      else:
        return False 
      result = []
      html = pageForAnalyze[1]
      tagResult = []
      for z in self.tagContent:
        if len(z) >= 2:
          tempResult = html.find_all(z[0])
          for x in tempResult:
            for key,val in z[1].items():
              if val in x.get(key):
                result.append(x)
        elif len(z) == 1:
          result = html.find_all(z[0])
          for y in result:
            result.append(y)
        for x in result:
          if len(z) == 3:
            if z[3] == 'text':
              tagResult = x.get_text()
            else:
              for x in result:
                tagResult = x.get(z[3])
        if tagResult != []: 
          arrayResult= [pageForAnalyze[0], tagResult]
        else:
          arrayResult= [pageForAnalyze[0], result]
      self.content.append(arrayResult)
    except KeyError:
      return False
    else:
      return True
      
  
  
  def GetPages(self):
    try:
      tempUrl = []
      tempPages = []
      thisLayer = 0
      self.n1Url = len(self.url)
      for _ in range(0, self.n1Url):
        try:
          self.nUrl = len(self.url)
          for __ in range(0, self.nUrl):
            try:
              tempUrl = self.url.pop()
            except KeyError:
              break
            else:
              tempPages = []
              if tempUrl not in self.urls_ja_escaneadas:
                
                self.urls_ja_escaneadas.append(tempUrl)
                resposta = requests.get(tempUrl)
                if resposta.status_code == 200:
                  thisLayer += 1
                  try:
                    tempPages.append(tempUrl)
                    try:
                      tempPages.append(BeautifulSoup(resposta.content, "html.parser"))
                    except TypeError:
                      pass
                    else:
                      self.pages.append(tempPages)   
                      bs = BeautifulSoup(resposta.content, "html.parser")
                      for link in bs.find_all('a'):
                        if link["href"].startswith(self.urlInicial):
                          self.url.append(link["href"])
                        elif link["href"].startswith("/") and tempUrl+link["href"] != tempUrl+"/":
                          self.url.append(tempUrl + link["href"])
                        elif link["href"].startswith(tempUrl) and tempUrl+link["href"] != tempUrl+"/":
                          self.url.append(link["href"])
                  except KeyError:
                    pass

        except KeyError:
          pass
      self.deep.append(thisLayer)
    except KeyError:
      return False    
    
  def tostring(self):
    for x in range(0, len(self.content)):
      self.content.insert(x, str(self.content[x]))
